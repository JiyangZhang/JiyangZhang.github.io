%% This file is generated by script wriiten by Jiyang

@String{ASE = "International Conference on Automated Software Engineering (ASE)"}
@String{ACL = "Annual Meeting of the Association for Computational Linguistics (ACL)"}
@String{ESECFSE-DEMO = "International Symposium on the Foundations of Software Engineering (FSE Demonstrations Track)"}
@String{AST = "International Conference on Automation of Software Test (AST)"}
@String{ICSE-SEIP = "International Conference on Software Engineering (ICSE Software Engineering in Practice Track)"}
@String{ISSTA = "International Symposium on Software Testing and Analysis (ISSTA)"}
@String{FSE = "Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (FSE)"}
@String{CAP = "Workshop on Computer Assisted Programming (CAP)"}
@String{ICTIR = "International Conference on Theory of Information Retrieval (ICTIR)"}
    
@inproceedings{ZhangETALDeltaCode,
  title     = {1. Multilingual Code Co-Evolution Using Large Language Models},
  author    = {Jiyang Zhang and Pengyu Nie and Junyi Jessy Li and Milos Gligoric},
  booktitle = FSE # { to appear},
  year      = {2023},
  pdf       = {papers/DeltaCodeTranslation.pdf},
  slides    = {slides/FSE23.pdf},
  abstract  = {Many software projects implement APIs and algorithms in multiple programming languages. Maintaining such projects is tiresome, as developers have to ensure that any change (e.g., a bug fix or a new feature) is being propagated, timely and without errors, to implementations in other programming languages. In the world of ever-changing software, using rule-based translation tools (i.e., transpilers) or machine learning models for translating code from one language to another provides limited value. Translating each time the entire codebase from one language to another is not the way developers work. In this paper, we target a novel task: translating code changes from one programming language to another using large language models (LLMs). We design and implement the first LLM, dubbed Codeditor, to tackle this task. Codeditor explicitly models code changes as edit sequences and learns to correlate changes across programming languages. To evaluate Codeditor, we collect a corpus of 6,613 aligned code changes from 8 pairs of open-source software projects implementing similar functionalities in two programming languages (Java and C#). Results show that Codeditor outperforms the state-of-the-art approaches by a large margin on all commonly used automatic metrics. Our work also reveals that Codeditor is complementary to the existing generation-based models, and their combination ensures even greater performance.},
  code      = {https://github.com/EngineeringSoftware/codeditor}
}

@inproceedings{ZhangETAL23Using,
  title     = {2. Using Large-scale Heterogeneous Graph Representation Learning for Code Review Recommendations at Microsoft},
  author    = {Jiyang Zhang and Maddila, Chandra and Bairi, Ram and Bird, Christian and Raizada, Ujjwal and Agrawal, Apoorva and Jhawar, Yamini and Herzig, Kim and van Deursen, Arie},
  booktitle = ICSE-SEIP,
  year      = {2023},
  pdf       = {papers/GNNReview.pdf}
}

@inproceedings{ZhangETAL23More,
  title     = {3. More Precise Regression Test Selection via Reasoning about Semantics-Modifying Changes},
  author    = {Liu, Yu and Jiyang Zhang and Nie, Pengyu and Gligoric, Milos and Legunsen, Owolabi},
  booktitle = ISSTA,
  year      = {2023},
  pdf       = {papers/FineRTS.pdf},
  slides    = {slides/ISSTA23.pdf},
  code      = {https://github.com/EngineeringSoftware/FineRTS}
}

@inproceedings{ZhangETAL22CoditT5,
  title     = {4. CoditT5: Pretraining for Source Code and Natural Language Editing},
  author    = {Jiyang Zhang and Panthaplackel, Sheena and Nie, Pengyu and Li, Junyi Jessy and Mooney, Raymond and Gligoric, Milos},
  booktitle = ASE,
  year      = {2022},
  pdf       = {papers/CoditT5.pdf},
  slides    = {slides/ASE22.pdf},
  code      = {https://github.com/EngineeringSoftware/CoditT5}
}

@inproceedings{ZhangETAL22Python,
  title     = {5. Python-by-Contract Dataset},
  author    = {Jiyang Zhang and Ristin, Marko and Phillip, Schanely and Wernher van de Venn, Hans and Gligoric, Milos},
  booktitle = ESECFSE-DEMO,
  year      = {2022},
  pdf       = {papers/PythonContract.pdf},
  code      = {https://github.com/mristin/python-by-contract-corpus},
  video     = {https://www.youtube.com/watch?v=08wZN-xh6mY}
}

@inproceedings{NieETAL22Impact,
  title     = {6. Impact of evaluation methodologies on code summarization},
  author    = {Nie, Pengyu and Jiyang Zhang and Li, Junyi Jessy and Mooney, Raymond and Gligoric, Milos},
  booktitle = ACL,
  year      = {2022},
  pdf       = {papers/NLPEvolv.pdf},
  code      = {https://github.com/EngineeringSoftware/time-segmented-evaluation}
}

@inproceedings{ZhangETAL22Comparing,
  title     = {7. Comparing and Combining Analysis-Based and Learning-Based Regression Test Selection},
  author    = {Jiyang Zhang and Liu, Yu and Gligoric, Milos and Legunsen, Owolabi and Shi, August},
  booktitle = AST,
  year      = {2022},
  pdf       = {papers/MLRTS.pdf},
  slides    = {slides/AST22.pdf},
  code      = {https://github.com/EngineeringSoftware/predictiverts}
}

@inproceedings{ZhangETAL20Leveraging,
  title     = {8. Leveraging class hierarchy for code comprehension},
  author    = {Jiyang Zhang and Panthaplackel, Sheena and Nie, Pengyu and Li, Junyi Jessy and Mooney, Raymond J. and Gligoric, Milos},
  booktitle = CAP,
  year      = {2020},
  pdf       = {papers/HieComment.pdf}
}

@inproceedings{NieETAL19Integrated,
  title     = {9. Integrated Learning of Features and Ranking Function in Information Retrieval},
  author    = {Nie, Yifan and Jiyang Zhang and Nie, Jian-Yun},
  booktitle = ICTIR,
  year      = {2019},
  pdf       = {papers/IR.pdf}
}